{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from R3GAN.Networks import Generator,Discriminator\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import  Path\n",
    "from training.loss import R3GANLoss\n",
    "from dnnlib import EasyDict\n",
    "import numpy as np  \n",
    "from time import time\n",
    "from torch_utils import training_stats,misc\n",
    "import dnnlib\n",
    "import torchmetrics\n",
    "from torch_utils.ops import conv2d_gradfix\n",
    "from torch_utils.ops import grid_sample_gradfix\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageUpsampler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageUpsampler, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample1 = nn.ConvTranspose2d(\n",
    "            128, 128, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x64 -> 64x128\n",
    "        self.upsample2 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x128 -> 64x256\n",
    "        self.upsample3 = nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x256 -> 64x512\n",
    "        self.upsample4 = nn.ConvTranspose2d(32, 1, kernel_size=1)  # 64x512 -> 64x576\n",
    "\n",
    "        self.final_conv = nn.Linear(512, 576)  # Refinement\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.relu(self.upsample1(x))\n",
    "        x = F.relu(self.upsample2(x))\n",
    "        x = F.relu(self.upsample3(x))\n",
    "        x = F.relu(self.upsample4(x))\n",
    "\n",
    "        x = self.final_conv(x)  # Output single-channel\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c_dim=0\n",
    "        self.z_dim=256\n",
    "        self.generator = Generator(\n",
    "            NoiseDimension=256,\n",
    "            WidthPerStage=[128,64,32,16,1],\n",
    "            CardinalityPerStage=[1,1,1,1,1],\n",
    "            BlocksPerStage=[2 * x for x in [1, 1, 1, 1, 1]],\n",
    "            ExpansionFactor=2,\n",
    "            ConditionDimension=None,\n",
    "            ConditionEmbeddingDimension=0,\n",
    "            KernelSize=3,\n",
    "            ResamplingFilter=[1, 2, 1],\n",
    "        )\n",
    "        self.upsampler = ImageUpsampler()\n",
    "\n",
    "    def forward(self, x,c=None):\n",
    "        x = self.generator(x)\n",
    "        x = self.upsampler(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDownsampler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageDownsampler, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample1 = nn.Conv2d(\n",
    "            128, 128, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x64 -> 64x128\n",
    "        self.upsample2 = nn.Conv2d(\n",
    "            64, 128, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x128 -> 64x256\n",
    "        self.upsample3 = nn.Conv2d(\n",
    "            32, 64, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x256 -> 64x512\n",
    "        self.upsample4 = nn.Conv2d(1, 32, kernel_size=1)  # 64x512 -> 64x576\n",
    "\n",
    "        self.final_conv = nn.Linear(576, 512)  # Refinement\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.final_conv(x)\n",
    "        x = F.relu(self.upsample4(x))\n",
    "        x = F.relu(self.upsample3(x))\n",
    "        x = F.relu(self.upsample2(x))\n",
    "        x = F.relu(self.upsample1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminator = Discriminator(\n",
    "            WidthPerStage=[1, 16, 32, 64, 128],\n",
    "            CardinalityPerStage=[1, 1, 1, 1],\n",
    "            BlocksPerStage=[2, 2, 2, 2],\n",
    "            ExpansionFactor=2,\n",
    "            ConditionDimension=None,\n",
    "            ConditionEmbeddingDimension=0,\n",
    "            KernelSize=3,\n",
    "            ResamplingFilter=[1, 2, 1],\n",
    "        )\n",
    "        self.imagedownsampler=ImageDownsampler()\n",
    "    \n",
    "    def forward(self,x,c=None):\n",
    "        x=self.imagedownsampler(x)\n",
    "        x=self.discriminator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedBatchDataset(Dataset):\n",
    "    def __init__(self, embedding_path,hidden_state_path, fixed_batch_size, transform=None, drop_last=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory containing .pt files.\n",
    "            fixed_batch_size (int): The fixed number of examples per output sample.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            drop_last (bool): If True, drop leftover samples that donâ€™t form a full batch.\n",
    "                               If False, pad them with zeros to create a full batch.\n",
    "        \"\"\"\n",
    "        self.embedding_path =embedding_path\n",
    "        self.hidden_state_path =hidden_state_path\n",
    "        self.fixed_batch_size = fixed_batch_size\n",
    "        self.transform = transform\n",
    "        self.image_shape=[1,64,576]\n",
    "        self.num_channels=1\n",
    "        self.resolution=64\n",
    "        self.has_labels=False\n",
    "        self.has_onehot_labels=False\n",
    "        self.label_shape=1\n",
    "        self.label_dim=0\n",
    "        self.name=\"something\"\n",
    "        \n",
    "        # Find all .pt files in the root directory.\n",
    "        self.embedding_files = [os.path.join(embedding_path, fname)\n",
    "                      for fname in os.listdir(embedding_path) if fname.endswith('.pt')]\n",
    "\n",
    "        # Build an index mapping from dataset index to (file_index, start_index)\n",
    "        # Each file might yield multiple chunks.\n",
    "        self.index_mapping = []\n",
    "        for file_index, file_path in enumerate(self.embedding_files):\n",
    "            # Load the tensor to know the number of samples.\n",
    "            tensor = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "            num_samples = tensor.size(0)\n",
    "            # Determine how many full fixed-size groups can be obtained.\n",
    "            num_groups = num_samples // fixed_batch_size\n",
    "            \n",
    "            # For each full group, remember the starting index.\n",
    "            for group in range(num_groups):\n",
    "                start_index = group * fixed_batch_size\n",
    "                self.index_mapping.append((file_index, start_index))\n",
    "            \n",
    "            # If drop_last is False and there are leftover samples\n",
    "            # store the starting index of the remainder for later padding.\n",
    "            if not drop_last and (num_samples % fixed_batch_size > 0):\n",
    "                start_index = num_groups * fixed_batch_size\n",
    "                self.index_mapping.append((file_index, start_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_index, start_index = self.index_mapping[idx]\n",
    "        file_path = self.embedding_files[file_index]\n",
    "        # Load the entire tensor stored in the file.\n",
    "        embeddings = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "        file_name=Path(file_path).stem\n",
    "        hidden_state_path=os.path.join(self.hidden_state_path,file_name+\".pt\")\n",
    "        hidden_state = torch.load(hidden_state_path, map_location=torch.device('cpu'))\n",
    "        # Slice out the chunk that we want.\n",
    "        batch_embedding = embeddings[start_index : start_index + self.fixed_batch_size]\n",
    "        batch_hidden_state = hidden_state[start_index : start_index + self.fixed_batch_size]\n",
    "        # pad the batch along dimension 0 with zeros.\n",
    "        if batch_embedding.size(0) < self.fixed_batch_size:\n",
    "            pad_amount = self.fixed_batch_size - batch_embedding.size(0)\n",
    "            pad_tensor = torch.zeros((pad_amount,) + batch_embedding.shape[1:], dtype=batch_embedding.dtype)\n",
    "            batch_embedding = torch.cat([batch_embedding, pad_tensor], dim=0)\n",
    "            batch_hidden_state = torch.cat([batch_hidden_state, pad_tensor], dim=0)\n",
    "        return batch_embedding.to(torch.float32),batch_hidden_state.to(torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=GeneratorModel().to('cuda')\n",
    "discriminator=DiscriminatorModel().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_g=torch.optim.Adam(generator.parameters(),betas=[0.0,0.0])\n",
    "opt_d=torch.optim.Adam(discriminator.parameters(),betas=[0.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=R3GANLoss(G=generator,D=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_kwargs={\"embedding_path\":\"/mnt/d/work/R3GAN/embeddings\",\"hidden_state_path\":\"/mnt/d/work/R3GAN/hidden_states\",\"fixed_batch_size\": 4}\n",
    "train_dataset=FixedBatchDataset(**training_set_kwargs)\n",
    "training_set_sampler = misc.InfiniteSampler(dataset=train_dataset, rank=0, num_replicas=1, seed=0)\n",
    "train_dataloader=iter(DataLoader(dataset=train_dataset,batch_size=1,sampler=training_set_sampler))\n",
    "test_dataset=FixedBatchDataset(**training_set_kwargs)\n",
    "test_dataloader=DataLoader(dataset=test_dataset,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_decay_with_warmup(cur_nimg, base_value, total_nimg, final_value=0.0, warmup_value=0.0, warmup_nimg=0, hold_base_value_nimg=0):\n",
    "    decay = 0.5 * (1 + np.cos(np.pi * (cur_nimg - warmup_nimg - hold_base_value_nimg) / float(total_nimg - warmup_nimg - hold_base_value_nimg)))\n",
    "    cur_value = base_value + (1 - decay) * (final_value - base_value)\n",
    "    if hold_base_value_nimg > 0:\n",
    "        cur_value = np.where(cur_nimg > warmup_nimg + hold_base_value_nimg, cur_value, base_value)\n",
    "    if warmup_nimg > 0:\n",
    "        slope = (base_value - warmup_value) / warmup_nimg\n",
    "        warmup_v = slope * cur_nimg + warmup_value\n",
    "        cur_value = np.where(cur_nimg < warmup_nimg, warmup_v, cur_value)\n",
    "    return float(np.where(cur_nimg > total_nimg, final_value, cur_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(real_images, generated_images):\n",
    "    \"\"\"\n",
    "    real_images: Tensor of shape (batch_size, 1, 64, 576)\n",
    "    generated_images: Tensor of shape (batch_size, 1, 64, 576)\n",
    "    \n",
    "    Returns:\n",
    "    Mean cosine similarity score for the batch\n",
    "    \"\"\"\n",
    "    # Flatten the images to (batch_size, features)\n",
    "    real_flattened = real_images.view(real_images.size(0), -1)  # Shape: (batch_size, 64*576)\n",
    "    generated_flattened = generated_images.view(generated_images.size(0), -1)  # Shape: (batch_size, 64*576)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = torchmetrics.functional.pairwise_cosine_similarity(real_flattened, generated_flattened)\n",
    "\n",
    "    # Return mean cosine similarity\n",
    "    return cosine_similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7499)\n"
     ]
    }
   ],
   "source": [
    "metric=torchmetrics.CosineSimilarity(reduction=\"mean\")\n",
    "for i in range(10):\n",
    "    a=torch.rand(size=[2,1,64,576]).flatten(start_dim=0,end_dim=2)\n",
    "    b=torch.rand(size=[2,1,64,576]).flatten(start_dim=0,end_dim=2)\n",
    "    # acc=metric(a,b)\n",
    "    metric.update(a,b)\n",
    "    # print(acc)\n",
    "print(metric.compute())\n",
    "metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_nimg=0\n",
    "ema_nimg = 5000 * 1000\n",
    "decay_nimg = 2e7\n",
    "lr_scheduler={ 'base_value': 2e-4, 'final_value': 5e-5, 'total_nimg': decay_nimg }\n",
    "beta2_scheduler={ 'base_value': 0.9, 'final_value': 0.99, 'total_nimg': decay_nimg }\n",
    "gamma_scheduler={ 'base_value': 0.05, 'final_value': 0.005, 'total_nimg': decay_nimg }\n",
    "ema_scheduler={ 'base_value': 0, 'final_value': ema_nimg, 'total_nimg': decay_nimg }\n",
    "\n",
    "cur_lr = cosine_decay_with_warmup(cur_nimg,**lr_scheduler)\n",
    "cur_beta2 = cosine_decay_with_warmup(cur_nimg, **beta2_scheduler)\n",
    "cur_gamma = cosine_decay_with_warmup(cur_nimg, **gamma_scheduler)\n",
    "cur_ema_nimg = cosine_decay_with_warmup(cur_nimg, **ema_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases=[]\n",
    "phases+=[EasyDict(name='D', module=discriminator, opt=opt_d, batch_gpu=4)]\n",
    "phases+=[EasyDict(name='G', module=generator, opt=opt_g, batch_gpu=4)]\n",
    "for phase in phases:\n",
    "    phase.start_event = torch.cuda.Event(enable_timing=True)\n",
    "    phase.end_event = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 25000 kimg...\n",
      "\n",
      "tensor(-0.0207, device='cuda:0')\n",
      "tick 0     kimg 0.0      time 2s           sec/tick 0.7     sec/kimg 175.92  maintenance 1.0    cpumem 1.32   gpumem 0.40   reserved 0.49  \n",
      "tensor(-0.0206, device='cuda:0')\n",
      "tick 0     kimg 0.0      time 5s           sec/tick 4.1     sec/kimg 512.18  maintenance 1.0    cpumem 1.46   gpumem 0.40   reserved 0.49  \n"
     ]
    }
   ],
   "source": [
    "num_gpus=1\n",
    "run_dir=\"./\"\n",
    "total_kimg=25000\n",
    "kimg_per_tick=4\n",
    "batch_size=4\n",
    "rank=0\n",
    "random_seed=0\n",
    "cudnn_benchmark=True\n",
    "start_time=time()\n",
    "tick_start_time = time()\n",
    "similarity_metric=torchmetrics.CosineSimilarity(reduction=\"mean\")\n",
    "device = torch.device('cuda', rank)\n",
    "# device=torch.device(\"cpu\")\n",
    "np.random.seed(random_seed * num_gpus + rank)\n",
    "torch.manual_seed(random_seed * num_gpus + rank)\n",
    "torch.backends.cudnn.benchmark = cudnn_benchmark    # Improves training speed.\n",
    "torch.backends.cuda.matmul.allow_tf32 = False       # Improves numerical accuracy.\n",
    "torch.backends.cudnn.allow_tf32 = False             # Improves numerical accuracy.\n",
    "conv2d_gradfix.enabled = True                       # Improves training speed.\n",
    "grid_sample_gradfix.enabled = True                  # Avoids errors with the augmentation pipe.\n",
    "stats_collector = training_stats.Collector(regex='.*')\n",
    "stats_metrics = dict()\n",
    "stats_jsonl = None\n",
    "stats_tfevents = None\n",
    "if rank == 0:\n",
    "        stats_jsonl = open(os.path.join(run_dir, 'stats.jsonl'), 'wt')\n",
    "        try:\n",
    "            import torch.utils.tensorboard as tensorboard\n",
    "            stats_tfevents = tensorboard.SummaryWriter(run_dir)\n",
    "        except ImportError as err:\n",
    "            print('Skipping tfevents export:', err)\n",
    "\n",
    "# Train.\n",
    "if rank == 0:\n",
    "    print(f'Training for {total_kimg} kimg...')\n",
    "    print()\n",
    "cur_nimg = 0\n",
    "cur_tick = 0\n",
    "tick_start_nimg = cur_nimg\n",
    "tick_start_time = time()\n",
    "maintenance_time = tick_start_time - start_time\n",
    "batch_idx = 0\n",
    "for phase in phases:\n",
    "    if phase.start_event is not None:\n",
    "        phase.start_event.record(torch.cuda.current_stream(device))\n",
    "    if phase.end_event is not None:\n",
    "        phase.end_event.record(torch.cuda.current_stream(device))\n",
    "for _ in range(2):\n",
    "    \n",
    "    with torch.autograd.profiler.record_function(\"data_fetch\"):\n",
    "        D_z, D_img = next(train_dataloader)\n",
    "        D_z, D_img= D_z.squeeze(0).unsqueeze(1), D_img.squeeze(0).unsqueeze(1)\n",
    "        G_z,G_img = next(train_dataloader)\n",
    "        G_z,G_img=G_z.squeeze(0).unsqueeze(1),G_img.squeeze(0).unsqueeze(1)\n",
    "        all_real_img = []\n",
    "        all_gen_z = []\n",
    "        all_real_img += [(D_img.detach().clone().to(device).to(torch.float32)).split(batch_size)]\n",
    "        all_gen_z += [D_z.detach().clone().to(device).split(batch_size)]\n",
    "        all_real_img += [(G_img.detach().clone().to(device).to(torch.float32)).split(batch_size)]\n",
    "        all_gen_z += [G_z.detach().clone().to(device).split(batch_size)]\n",
    "    cur_lr = cosine_decay_with_warmup(cur_nimg,**lr_scheduler)\n",
    "    cur_beta2 = cosine_decay_with_warmup(cur_nimg, **beta2_scheduler)\n",
    "    cur_gamma = cosine_decay_with_warmup(cur_nimg, **gamma_scheduler)\n",
    "    cur_ema_nimg = cosine_decay_with_warmup(cur_nimg, **ema_scheduler)\n",
    "\n",
    "    for phase, phase_gen_z, phase_real_img in zip(phases, all_gen_z, all_real_img):\n",
    "        if phase.start_event is not None:\n",
    "            phase.start_event.record(torch.cuda.current_stream(device))\n",
    "\n",
    "        # Accumulate gradients.\n",
    "        phase.opt.zero_grad(set_to_none=True)\n",
    "        phase.module.requires_grad_(True)\n",
    "        for real_img, gen_z in zip(phase_real_img, phase_gen_z):\n",
    "            loss_func.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=None, gen_z=gen_z, gamma=cur_gamma, gain=1)\n",
    "        phase.module.requires_grad_(False)\n",
    "\n",
    "        # Update weights.  \n",
    "        for g in phase.opt.param_groups:\n",
    "            g['lr'] = cur_lr\n",
    "            g['betas'] = (0, cur_beta2)\n",
    "                    \n",
    "        with torch.autograd.profiler.record_function(phase.name + '_opt'):\n",
    "            params = [param for param in phase.module.parameters() if param.grad is not None]\n",
    "            if len(params) > 0:\n",
    "                flat = torch.cat([param.grad.flatten() for param in params])\n",
    "                if num_gpus > 1:\n",
    "                    torch.distributed.all_reduce(flat)\n",
    "                    flat /= num_gpus\n",
    "                grads = flat.split([param.numel() for param in params])\n",
    "                for param, grad in zip(params, grads):\n",
    "                    param.grad = grad.reshape(param.shape)\n",
    "            phase.opt.step()\n",
    "\n",
    "        # Phase done.\n",
    "        if phase.end_event is not None:\n",
    "            phase.end_event.record(torch.cuda.current_stream(device))\n",
    "    done = (cur_nimg >= total_kimg * 1000)\n",
    "    if (not done) and (cur_tick != 0) and (cur_nimg < tick_start_nimg + kimg_per_tick * 1000):\n",
    "        continue\n",
    "    cur_nimg += batch_size\n",
    "    tick_end_time = time()\n",
    "    fields = []\n",
    "    fields += [f\"tick {training_stats.report0('Progress/tick', cur_tick):<5d}\"]\n",
    "    fields += [f\"kimg {training_stats.report0('Progress/kimg', cur_nimg / 1e3):<8.1f}\"]\n",
    "    fields += [f\"time {dnnlib.util.format_time(training_stats.report0('Timing/total_sec', tick_end_time - start_time)):<12s}\"]\n",
    "    fields += [f\"sec/tick {training_stats.report0('Timing/sec_per_tick', tick_end_time - tick_start_time):<7.1f}\"]\n",
    "    fields += [f\"sec/kimg {training_stats.report0('Timing/sec_per_kimg', (tick_end_time - tick_start_time) / (cur_nimg - tick_start_nimg) * 1e3):<7.2f}\"]\n",
    "    fields += [f\"maintenance {training_stats.report0('Timing/maintenance_sec', maintenance_time):<6.1f}\"]\n",
    "    fields += [f\"cpumem {training_stats.report0('Resources/cpu_mem_gb', psutil.Process(os.getpid()).memory_info().rss / 2**30):<6.2f}\"]\n",
    "    fields += [f\"gpumem {training_stats.report0('Resources/peak_gpu_mem_gb', torch.cuda.max_memory_allocated(device) / 2**30):<6.2f}\"]\n",
    "    fields += [f\"reserved {training_stats.report0('Resources/peak_gpu_mem_reserved_gb', torch.cuda.max_memory_reserved(device) / 2**30):<6.2f}\"]\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    # fields += [f\"augment {training_stats.report0('Progress/augment', float(augment_pipe.p.cpu()) if augment_pipe is not None else 0):.3f}\"]\n",
    "    training_stats.report0('Progress/lr', cur_lr)\n",
    "    training_stats.report0('Progress/ema_mimg', cur_ema_nimg / 1e6)\n",
    "    training_stats.report0('Progress/beta2', cur_beta2)\n",
    "    training_stats.report0('Progress/gamma', cur_gamma)\n",
    "    training_stats.report0('Timing/total_hours', (tick_end_time - start_time) / (60 * 60))\n",
    "    training_stats.report0('Timing/total_days', (tick_end_time - start_time) / (24 * 60 * 60))\n",
    "    with torch.no_grad():\n",
    "        for embedding,hidden_state in test_dataloader:\n",
    "            generated_hidden_state=generator(embedding.squeeze(0).unsqueeze(1).to(device))\n",
    "            # simi=similarity_metric(generated_hidden_state.flatten(start_dim=0,end_dim=2),hidden_state.squeeze(0).unsqueeze(1).to(device).flatten(start_dim=0,end_dim=2))\n",
    "            similarity_metric.update(generated_hidden_state.flatten(start_dim=0,end_dim=2),hidden_state.squeeze(0).unsqueeze(1).to(device).flatten(start_dim=0,end_dim=2))\n",
    "    # fields += [f\"similarity {training_stats.report0('Progress/tick', similarity_metric.compute().item()):<5d}\"]\n",
    "    acc=similarity_metric.compute()\n",
    "    print(acc)\n",
    "    similarity_metric.reset()\n",
    "    if rank == 0:\n",
    "        print(' '.join(fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r3gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
