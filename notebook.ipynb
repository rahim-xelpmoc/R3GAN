{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/R3GAN/') # Replace with the actual path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from R3GAN.Networks import Generator,Discriminator\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import  Path\n",
    "from training.loss import R3GANLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageUpsampler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageUpsampler, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample1 = nn.ConvTranspose2d(\n",
    "            128, 128, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x64 -> 64x128\n",
    "        self.upsample2 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x128 -> 64x256\n",
    "        self.upsample3 = nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x256 -> 64x512\n",
    "        self.upsample4 = nn.ConvTranspose2d(32, 1, kernel_size=1)  # 64x512 -> 64x576\n",
    "\n",
    "        self.final_conv = nn.Linear(512, 576)  # Refinement\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.relu(self.upsample1(x))\n",
    "        x = F.relu(self.upsample2(x))\n",
    "        x = F.relu(self.upsample3(x))\n",
    "        x = F.relu(self.upsample4(x))\n",
    "\n",
    "        x = self.final_conv(x)  # Output single-channel\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c_dim=0\n",
    "        self.z_dim=256\n",
    "        self.generator = Generator(\n",
    "            NoiseDimension=256,\n",
    "            WidthPerStage=[128, 64, 32, 16, 1],\n",
    "            CardinalityPerStage=[1, 1, 1, 1, 1],\n",
    "            BlocksPerStage=[2, 2, 2, 2, 2],\n",
    "            ExpansionFactor=2,\n",
    "            ConditionDimension=None,\n",
    "            ConditionEmbeddingDimension=0,\n",
    "            KernelSize=3,\n",
    "            ResamplingFilter=[1, 2, 1],\n",
    "        )\n",
    "        self.upsampler = ImageUpsampler()\n",
    "\n",
    "    def forward(self, x,c=None):\n",
    "        x = self.generator(x)\n",
    "        x = self.upsampler(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDownsampler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageDownsampler, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample1 = nn.Conv2d(\n",
    "            128, 128, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x64 -> 64x128\n",
    "        self.upsample2 = nn.Conv2d(\n",
    "            64, 128, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x128 -> 64x256\n",
    "        self.upsample3 = nn.Conv2d(\n",
    "            32, 64, kernel_size=(1, 2), stride=(1, 2)\n",
    "        )  # 64x256 -> 64x512\n",
    "        self.upsample4 = nn.Conv2d(1, 32, kernel_size=1)  # 64x512 -> 64x576\n",
    "\n",
    "        self.final_conv = nn.Linear(576, 512)  # Refinement\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.final_conv(x)\n",
    "        x = F.relu(self.upsample4(x))\n",
    "        x = F.relu(self.upsample3(x))\n",
    "        x = F.relu(self.upsample2(x))\n",
    "        x = F.relu(self.upsample1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminator = Discriminator(\n",
    "            WidthPerStage=[1, 16, 32, 64, 128],\n",
    "            CardinalityPerStage=[1, 1, 1, 1],\n",
    "            BlocksPerStage=[2, 2, 2, 2],\n",
    "            ExpansionFactor=2,\n",
    "            ConditionDimension=None,\n",
    "            ConditionEmbeddingDimension=0,\n",
    "            KernelSize=3,\n",
    "            ResamplingFilter=[1, 2, 1],\n",
    "        )\n",
    "        self.imagedownsampler=ImageDownsampler()\n",
    "    \n",
    "    def forward(self,x,c=None):\n",
    "        x=self.imagedownsampler(x)\n",
    "        x=self.discriminator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FixedBatchDataset(Dataset):\n",
    "    def __init__(self, embedding_path,hidden_state_path, fixed_batch_size, transform=None, drop_last=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory containing .pt files.\n",
    "            fixed_batch_size (int): The fixed number of examples per output sample.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            drop_last (bool): If True, drop leftover samples that donâ€™t form a full batch.\n",
    "                               If False, pad them with zeros to create a full batch.\n",
    "        \"\"\"\n",
    "        self.embedding_path =embedding_path\n",
    "        self.hidden_state_path =hidden_state_path\n",
    "        self.fixed_batch_size = fixed_batch_size\n",
    "        self.transform = transform\n",
    "        self.image_shape=[64,576]\n",
    "        self.num_channels=1\n",
    "        self.resolution=64\n",
    "        self.has_labels=False\n",
    "        self.has_onehot_labels=False\n",
    "        self.label_shape=1\n",
    "        self.label_dim=0\n",
    "        self.name=\"something\"\n",
    "        \n",
    "        # Find all .pt files in the root directory.\n",
    "        self.embedding_files = [os.path.join(embedding_path, fname)\n",
    "                      for fname in os.listdir(embedding_path) if fname.endswith('.pt')]\n",
    "\n",
    "        # Build an index mapping from dataset index to (file_index, start_index)\n",
    "        # Each file might yield multiple chunks.\n",
    "        self.index_mapping = []\n",
    "        for file_index, file_path in enumerate(self.embedding_files):\n",
    "            # Load the tensor to know the number of samples.\n",
    "            tensor = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "            num_samples = tensor.size(0)\n",
    "            # Determine how many full fixed-size groups can be obtained.\n",
    "            num_groups = num_samples // fixed_batch_size\n",
    "            \n",
    "            # For each full group, remember the starting index.\n",
    "            for group in range(num_groups):\n",
    "                start_index = group * fixed_batch_size\n",
    "                self.index_mapping.append((file_index, start_index))\n",
    "            \n",
    "            # If drop_last is False and there are leftover samples\n",
    "            # store the starting index of the remainder for later padding.\n",
    "            if not drop_last and (num_samples % fixed_batch_size > 0):\n",
    "                start_index = num_groups * fixed_batch_size\n",
    "                self.index_mapping.append((file_index, start_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_index, start_index = self.index_mapping[idx]\n",
    "        file_path = self.embedding_files[file_index]\n",
    "        # Load the entire tensor stored in the file.\n",
    "        embeddings = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "        file_name=Path(file_path).stem\n",
    "        hidden_state_path=os.path.join(self.hidden_state_path,file_name+\".pt\")\n",
    "        hidden_state = torch.load(hidden_state_path, map_location=torch.device('cpu'))\n",
    "        # Slice out the chunk that we want.\n",
    "        batch_embedding = embeddings[start_index : start_index + self.fixed_batch_size]\n",
    "        batch_hidden_state = hidden_state[start_index : start_index + self.fixed_batch_size]\n",
    "        # pad the batch along dimension 0 with zeros.\n",
    "        if batch_embedding.size(0) < self.fixed_batch_size:\n",
    "            pad_amount = self.fixed_batch_size - batch_embedding.size(0)\n",
    "            pad_tensor = torch.zeros((pad_amount,) + batch_embedding.shape[1:], dtype=batch_embedding.dtype)\n",
    "            batch_embedding = torch.cat([batch_embedding, pad_tensor], dim=0)\n",
    "            batch_hidden_state = torch.cat([batch_hidden_state, pad_tensor], dim=0)\n",
    "        return batch_embedding,batch_hidden_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=GeneratorModel()\n",
    "discriminator=DiscriminatorModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xelpmoc\\AppData\\Local\\Temp\\ipykernel_15480\\3596480369.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor = torch.load(file_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "training_set_kwargs={\"embedding_path\":\"D:/Xelpmoc/R3GAN/embeddings\",\"hidden_state_path\":\"D:/Xelpmoc/R3GAN/hidden_states\"}\n",
    "dataset=FixedBatchDataset(**training_set_kwargs,fixed_batch_size=4)\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xelpmoc\\AppData\\Local\\Temp\\ipykernel_15480\\3596480369.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(file_path, map_location=torch.device('cpu'))\n",
      "C:\\Users\\xelpmoc\\AppData\\Local\\Temp\\ipykernel_15480\\3596480369.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hidden_state = torch.load(hidden_state_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 256])\n",
      "torch.Size([4, 1, 64, 576])\n",
      "torch.Size([4, 1, 64, 576])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for embeddings,hidden_states in dataloader:\n",
    "    print(embeddings.squeeze(0).unsqueeze(1).shape)\n",
    "    print(hidden_states.squeeze(0).unsqueeze(1).shape)\n",
    "    out=generator(embeddings.squeeze(0).unsqueeze(1))\n",
    "    out2=discriminator(out)\n",
    "    print(out.shape)\n",
    "    print(out2.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_opt=torch.optim.Adam(generator.parameters(),betas=[0,0])\n",
    "d_opt=torch.optim.Adam(discriminator.parameters(),betas=[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=R3GANLoss(G=generator,D=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import psutil\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import dnnlib\n",
    "from torch_utils import misc\n",
    "from torch_utils import training_stats\n",
    "from torch_utils.ops import conv2d_gradfix\n",
    "from torch_utils.ops import grid_sample_gradfix\n",
    "\n",
    "import legacy\n",
    "from metrics import metric_main\n",
    "\n",
    "def cosine_decay_with_warmup(cur_nimg, base_value, total_nimg, final_value=0.0, warmup_value=0.0, warmup_nimg=0, hold_base_value_nimg=0):\n",
    "    decay = 0.5 * (1 + np.cos(np.pi * (cur_nimg - warmup_nimg - hold_base_value_nimg) / float(total_nimg - warmup_nimg - hold_base_value_nimg)))\n",
    "    cur_value = base_value + (1 - decay) * (final_value - base_value)\n",
    "    if hold_base_value_nimg > 0:\n",
    "        cur_value = np.where(cur_nimg > warmup_nimg + hold_base_value_nimg, cur_value, base_value)\n",
    "    if warmup_nimg > 0:\n",
    "        slope = (base_value - warmup_value) / warmup_nimg\n",
    "        warmup_v = slope * cur_nimg + warmup_value\n",
    "        cur_value = np.where(cur_nimg < warmup_nimg, warmup_v, cur_value)\n",
    "    return float(np.where(cur_nimg > total_nimg, final_value, cur_value))\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def setup_snapshot_image_grid(training_set, random_seed=0):\n",
    "    rnd = np.random.RandomState(random_seed)\n",
    "    gw = np.clip(7680 // training_set.image_shape[2], 7, 32)\n",
    "    gh = np.clip(4320 // training_set.image_shape[1], 4, 32)\n",
    "\n",
    "    # No labels => show random subset of training samples.\n",
    "    if not training_set.has_labels:\n",
    "        all_indices = list(range(len(training_set)))\n",
    "        rnd.shuffle(all_indices)\n",
    "        grid_indices = [all_indices[i % len(all_indices)] for i in range(gw * gh)]\n",
    "\n",
    "    else:\n",
    "        # Group training samples by label.\n",
    "        label_groups = dict() # label => [idx, ...]\n",
    "        for idx in range(len(training_set)):\n",
    "            label = tuple(training_set.get_details(idx).raw_label.flat[::-1])\n",
    "            if label not in label_groups:\n",
    "                label_groups[label] = []\n",
    "            label_groups[label].append(idx)\n",
    "\n",
    "        # Reorder.\n",
    "        label_order = sorted(label_groups.keys())\n",
    "        for label in label_order:\n",
    "            rnd.shuffle(label_groups[label])\n",
    "\n",
    "        # Organize into grid.\n",
    "        grid_indices = []\n",
    "        for y in range(gh):\n",
    "            label = label_order[y % len(label_order)]\n",
    "            indices = label_groups[label]\n",
    "            grid_indices += [indices[x % len(indices)] for x in range(gw)]\n",
    "            label_groups[label] = [indices[(i + gw) % len(indices)] for i in range(len(indices))]\n",
    "\n",
    "    # Load data.\n",
    "    images, labels = zip(*[training_set[i] for i in grid_indices])\n",
    "    return (gw, gh), np.stack(images), np.stack(labels)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def save_image_grid(img, fname, drange, grid_size):\n",
    "    lo, hi = drange\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    img = (img - lo) * (255 / (hi - lo))\n",
    "    img = np.rint(img).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    gw, gh = grid_size\n",
    "    _N, C, H, W = img.shape\n",
    "    img = img.reshape([gh, gw, C, H, W])\n",
    "    img = img.transpose(0, 3, 1, 4, 2)\n",
    "    img = img.reshape([gh * H, gw * W, C])\n",
    "\n",
    "    assert C in [1, 3]\n",
    "    if C == 1:\n",
    "        PIL.Image.fromarray(img[:, :, 0], 'L').save(fname)\n",
    "    if C == 3:\n",
    "        PIL.Image.fromarray(img, 'RGB').save(fname)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def remap_optimizer_state_dict(state_dict, device):\n",
    "    state_dict = copy.deepcopy(state_dict)\n",
    "    for param in state_dict['state'].values():\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "    return state_dict\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def training_loop(\n",
    "    run_dir                 = '.',      # Output directory.\n",
    "    training_set_kwargs     = {},       # Options for training set.\n",
    "    data_loader_kwargs      = {},       # Options for torch.utils.data.DataLoader.      # Options for discriminator network.\n",
    "    G_opt_kwargs            = {},       # Options for generator optimizer.\n",
    "    D_opt_kwargs            = {},       # Options for discriminator optimizer.\n",
    "    lr_scheduler            = None,\n",
    "    beta2_scheduler         = None,\n",
    "    augment_kwargs          = None,     # Options for augmentation pipeline. None = disable.\n",
    "    loss_kwargs             = {},       # Options for loss function.\n",
    "    gamma_scheduler         = None,\n",
    "    metrics                 = [],       # Metrics to evaluate during training.\n",
    "    random_seed             = 0,        # Global random seed.\n",
    "    num_gpus                = 1,        # Number of GPUs participating in the training.\n",
    "    rank                    = 0,        # Rank of the current process in [0, num_gpus[.\n",
    "    batch_size              = 4,        # Total batch size for one training iteration. Can be larger than batch_gpu * num_gpus.\n",
    "    g_batch_gpu             = 4,        # Number of samples processed at a time by one GPU.\n",
    "    d_batch_gpu             = 4,        # Number of samples processed at a time by one GPU.\n",
    "    ema_scheduler           = None,\n",
    "    aug_scheduler           = None,\n",
    "    total_kimg              = 25000,    # Total length of the training, measured in thousands of real images.\n",
    "    kimg_per_tick           = 4,        # Progress snapshot interval.\n",
    "    image_snapshot_ticks    = 50,       # How often to save image snapshots? None = disable.\n",
    "    network_snapshot_ticks  = 50,       # How often to save network snapshots? None = disable.\n",
    "    resume_pkl              = None,     # Network pickle to resume training from.\n",
    "    cudnn_benchmark         = True,     # Enable torch.backends.cudnn.benchmark?\n",
    "    abort_fn                = None,     # Callback function for determining whether to abort training. Must return consistent results across ranks.\n",
    "    progress_fn             = None,     # Callback function for updating training progress. Called for all ranks.\n",
    "):\n",
    "    # Initialize.\n",
    "    start_time = time.time()\n",
    "    # device = torch.device('cuda', rank)\n",
    "    device=torch.device(\"cpu\")\n",
    "    np.random.seed(random_seed * num_gpus + rank)\n",
    "    torch.manual_seed(random_seed * num_gpus + rank)\n",
    "    torch.backends.cudnn.benchmark = cudnn_benchmark    # Improves training speed.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False       # Improves numerical accuracy.\n",
    "    torch.backends.cudnn.allow_tf32 = False             # Improves numerical accuracy.\n",
    "    conv2d_gradfix.enabled = True                       # Improves training speed.\n",
    "    grid_sample_gradfix.enabled = True                  # Avoids errors with the augmentation pipe.\n",
    "\n",
    "    # Load training set.\n",
    "    if rank == 0:\n",
    "        print('Loading training set...')\n",
    "    training_set =FixedBatchDataset(embedding_path=training_set_kwargs['embedding_path'],hidden_state_path=training_set_kwargs['hidden_state_path'],fixed_batch_size=batch_size) # subclass of training.dataset.Dataset\n",
    "    training_set_sampler = misc.InfiniteSampler(dataset=training_set, rank=rank, num_replicas=num_gpus, seed=random_seed)\n",
    "    training_set_iterator = iter(torch.utils.data.DataLoader(dataset=training_set, sampler=training_set_sampler, batch_size=batch_size//num_gpus, **data_loader_kwargs))\n",
    "    if rank == 0:\n",
    "        print()\n",
    "        print('Num images: ', len(training_set))\n",
    "        print('Image shape:', training_set.image_shape)\n",
    "        print('Label shape:', training_set.label_shape)\n",
    "        print()\n",
    "\n",
    "    # Construct networks.\n",
    "    if rank == 0:\n",
    "        print('Constructing networks...')\n",
    "    common_kwargs = dict(c_dim=training_set.label_dim, img_resolution=training_set.resolution)\n",
    "    G = GeneratorModel().train().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
    "    D = DiscriminatorModel().train().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
    "    G_ema = copy.deepcopy(G).eval()\n",
    "\n",
    "    # Resume from existing pickle.\n",
    "    if resume_pkl is not None:\n",
    "        with dnnlib.util.open_url(resume_pkl) as f:\n",
    "            resume_data = legacy.load_network_pkl(f)\n",
    "        if rank == 0:\n",
    "            print(f'Resuming from \"{resume_pkl}\"')\n",
    "            for name, module in [('G', G), ('D', D), ('G_ema', G_ema)]:\n",
    "                misc.copy_params_and_buffers(resume_data[name], module, require_all=False)\n",
    "\n",
    "    # Print network summary tables.\n",
    "    if rank == 0:\n",
    "        z = torch.empty([min(g_batch_gpu, d_batch_gpu), G.z_dim], device=device)\n",
    "        c = torch.empty([min(g_batch_gpu, d_batch_gpu), G.c_dim], device=device)\n",
    "        img = misc.print_module_summary(G, [z, c])\n",
    "        misc.print_module_summary(D, [img, c])\n",
    "\n",
    "    # Setup augmentation.\n",
    "    if rank == 0:\n",
    "        print('Setting up augmentation...')\n",
    "    augment_pipe = None\n",
    "\n",
    "    if (augment_kwargs is not None) and (aug_scheduler is not None):\n",
    "        augment_pipe = dnnlib.util.construct_class_by_name(**augment_kwargs).train().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
    "        \n",
    "    # Distribute across GPUs.\n",
    "    if rank == 0:\n",
    "        print(f'Distributing across {num_gpus} GPUs...')\n",
    "    for module in [G, D, G_ema]:\n",
    "        if module is not None and num_gpus > 1:\n",
    "            for param in misc.params_and_buffers(module):\n",
    "                torch.distributed.broadcast(param, src=0)\n",
    "\n",
    "    # Setup training phases.\n",
    "    if rank == 0:\n",
    "        print('Setting up training phases...')\n",
    "    loss = dnnlib.util.construct_class_by_name(G=G, D=D, augment_pipe=augment_pipe, **loss_kwargs) # subclass of training.loss.Loss\n",
    "    phases = []\n",
    "    \n",
    "    opt = dnnlib.util.construct_class_by_name(params=D.parameters(), **D_opt_kwargs)\n",
    "    if resume_pkl is not None:\n",
    "        opt.load_state_dict(remap_optimizer_state_dict(resume_data['D_opt_state'], device))\n",
    "    phases += [dnnlib.EasyDict(name='D', module=D, opt=opt, batch_gpu=d_batch_gpu)]\n",
    "    \n",
    "    opt = dnnlib.util.construct_class_by_name(params=G.parameters(), **G_opt_kwargs)\n",
    "    if resume_pkl is not None:\n",
    "        opt.load_state_dict(remap_optimizer_state_dict(resume_data['G_opt_state'], device))\n",
    "    phases += [dnnlib.EasyDict(name='G', module=G, opt=opt, batch_gpu=g_batch_gpu)]\n",
    "    \n",
    "    for phase in phases:\n",
    "        phase.start_event = None\n",
    "        phase.end_event = None\n",
    "        if rank == 0:\n",
    "            phase.start_event = torch.cuda.Event(enable_timing=True)\n",
    "            phase.end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Export sample images.\n",
    "    grid_size = None\n",
    "    grid_z = None\n",
    "    grid_c = None\n",
    "    if rank == 0:\n",
    "        print('Exporting sample images...')\n",
    "        grid_size, images, labels = setup_snapshot_image_grid(training_set=training_set)\n",
    "        save_image_grid(images, os.path.join(run_dir, 'reals.png'), drange=[0,255], grid_size=grid_size)\n",
    "        grid_z = torch.randn([labels.shape[0], G.z_dim], device=device).split(g_batch_gpu)\n",
    "        grid_c = torch.from_numpy(labels).to(device).split(g_batch_gpu)\n",
    "        images = torch.cat([G_ema(z, c).cpu() for z, c in zip(grid_z, grid_c)]).to(torch.float).numpy()\n",
    "        save_image_grid(images, os.path.join(run_dir, 'fakes_init.png'), drange=[-1,1], grid_size=grid_size)\n",
    "\n",
    "    # Initialize logs.\n",
    "    if rank == 0:\n",
    "        print('Initializing logs...')\n",
    "    stats_collector = training_stats.Collector(regex='.*')\n",
    "    stats_metrics = dict()\n",
    "    stats_jsonl = None\n",
    "    stats_tfevents = None\n",
    "    if rank == 0:\n",
    "        stats_jsonl = open(os.path.join(run_dir, 'stats.jsonl'), 'wt')\n",
    "        try:\n",
    "            import torch.utils.tensorboard as tensorboard\n",
    "            stats_tfevents = tensorboard.SummaryWriter(run_dir)\n",
    "        except ImportError as err:\n",
    "            print('Skipping tfevents export:', err)\n",
    "\n",
    "    # Train.\n",
    "    if rank == 0:\n",
    "        print(f'Training for {total_kimg} kimg...')\n",
    "        print()\n",
    "    cur_nimg = resume_data['cur_nimg'] if resume_pkl is not None else 0\n",
    "    cur_tick = 0\n",
    "    tick_start_nimg = cur_nimg\n",
    "    tick_start_time = time.time()\n",
    "    maintenance_time = tick_start_time - start_time\n",
    "    batch_idx = 0\n",
    "    if progress_fn is not None:\n",
    "        progress_fn(0, total_kimg)\n",
    "        \n",
    "    # Dummy Timing, required to fix phase shift\n",
    "    for phase in phases:\n",
    "        if phase.start_event is not None:\n",
    "            phase.start_event.record(torch.cuda.current_stream(device))\n",
    "        if phase.end_event is not None:\n",
    "            phase.end_event.record(torch.cuda.current_stream(device))\n",
    "        \n",
    "    while True:\n",
    "        # Fetch training data.\n",
    "        with torch.autograd.profiler.record_function('data_fetch'):\n",
    "            D_z, D_img = next(training_set_iterator)\n",
    "            D_z, D_img= D_z.squeeze(0).unsqueeze(1), D_img.squeeze(0).unsqueeze(1)\n",
    "            # D_z = torch.randn([batch_size, G.z_dim], device=device)\n",
    "            D_img_c=torch.zeros(size=[batch_size])\n",
    "            \n",
    "            G_z,G_img = next(training_set_iterator)\n",
    "            G_z,G_img=G_z.squeeze(0).unsqueeze(1),G_img.squeeze(0).unsqueeze(1)\n",
    "            # G_z = torch.randn([batch_size, G.z_dim], device=device)\n",
    "            G_img_c=torch.zeros(size=[batch_size])\n",
    "            \n",
    "            all_real_img = []\n",
    "            all_real_c = []\n",
    "            all_gen_z = []\n",
    "            \n",
    "            # D\n",
    "            all_real_img += [(D_img.detach().clone().to(device).to(torch.float32)).split(d_batch_gpu)]\n",
    "            all_real_c += [D_img_c.detach().clone().to(device).split(d_batch_gpu)]\n",
    "            all_gen_z += [D_z.detach().clone().split(d_batch_gpu)]\n",
    "            \n",
    "            # G\n",
    "            all_real_img += [(G_img.detach().clone().to(device).to(torch.float32)).split(g_batch_gpu)]\n",
    "            all_real_c += [G_img_c.detach().clone().to(device).split(g_batch_gpu)]\n",
    "            all_gen_z += [G_z.detach().clone().split(g_batch_gpu)]\n",
    "            \n",
    "        cur_lr = cosine_decay_with_warmup(cur_nimg, **lr_scheduler)\n",
    "        cur_beta2 = cosine_decay_with_warmup(cur_nimg, **beta2_scheduler)\n",
    "        cur_gamma = cosine_decay_with_warmup(cur_nimg, **gamma_scheduler)\n",
    "        cur_ema_nimg = cosine_decay_with_warmup(cur_nimg, **ema_scheduler)\n",
    "        cur_aug_p = cosine_decay_with_warmup(cur_nimg, **aug_scheduler)\n",
    "        \n",
    "        if augment_pipe is not None:\n",
    "            augment_pipe.p.copy_(misc.constant(cur_aug_p, device=device))\n",
    "        \n",
    "        # Execute training phases.\n",
    "        for phase, phase_gen_z, phase_real_img, phase_real_c in zip(phases, all_gen_z, all_real_img, all_real_c):\n",
    "            if phase.start_event is not None:\n",
    "                phase.start_event.record(torch.cuda.current_stream(device))\n",
    "\n",
    "            # Accumulate gradients.\n",
    "            phase.opt.zero_grad(set_to_none=True)\n",
    "            phase.module.requires_grad_(True)\n",
    "            for real_img, real_c, gen_z in zip(phase_real_img, phase_real_c, phase_gen_z):\n",
    "                loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gamma=cur_gamma, gain=num_gpus * phase.batch_gpu / batch_size)\n",
    "            phase.module.requires_grad_(False)\n",
    "        \n",
    "            # Update weights.  \n",
    "            for g in phase.opt.param_groups:\n",
    "                g['lr'] = cur_lr\n",
    "                g['betas'] = (0, cur_beta2)\n",
    "                      \n",
    "            with torch.autograd.profiler.record_function(phase.name + '_opt'):\n",
    "                params = [param for param in phase.module.parameters() if param.grad is not None]\n",
    "                if len(params) > 0:\n",
    "                    flat = torch.cat([param.grad.flatten() for param in params])\n",
    "                    if num_gpus > 1:\n",
    "                        torch.distributed.all_reduce(flat)\n",
    "                        flat /= num_gpus\n",
    "                    grads = flat.split([param.numel() for param in params])\n",
    "                    for param, grad in zip(params, grads):\n",
    "                        param.grad = grad.reshape(param.shape)\n",
    "                phase.opt.step()\n",
    "\n",
    "            # Phase done.\n",
    "            if phase.end_event is not None:\n",
    "                phase.end_event.record(torch.cuda.current_stream(device))\n",
    "\n",
    "        # Update G_ema.\n",
    "        with torch.autograd.profiler.record_function('Gema'):\n",
    "            ema_beta = 0.5 ** (batch_size / max(cur_ema_nimg, 1e-8))\n",
    "            for p_ema, p in zip(G_ema.parameters(), G.parameters()):\n",
    "                p_ema.copy_(p.lerp(p_ema, ema_beta))\n",
    "            for b_ema, b in zip(G_ema.buffers(), G.buffers()):\n",
    "                b_ema.copy_(b)\n",
    "\n",
    "        # Update state.\n",
    "        cur_nimg += batch_size\n",
    "        batch_idx += 1\n",
    "\n",
    "        # Perform maintenance tasks once per tick.\n",
    "        done = (cur_nimg >= total_kimg * 1000)\n",
    "        if (not done) and (cur_tick != 0) and (cur_nimg < tick_start_nimg + kimg_per_tick * 1000):\n",
    "            continue\n",
    "\n",
    "        # Print status line, accumulating the same information in training_stats.\n",
    "        tick_end_time = time.time()\n",
    "        fields = []\n",
    "        fields += [f\"tick {training_stats.report0('Progress/tick', cur_tick):<5d}\"]\n",
    "        fields += [f\"kimg {training_stats.report0('Progress/kimg', cur_nimg / 1e3):<8.1f}\"]\n",
    "        fields += [f\"time {dnnlib.util.format_time(training_stats.report0('Timing/total_sec', tick_end_time - start_time)):<12s}\"]\n",
    "        fields += [f\"sec/tick {training_stats.report0('Timing/sec_per_tick', tick_end_time - tick_start_time):<7.1f}\"]\n",
    "        fields += [f\"sec/kimg {training_stats.report0('Timing/sec_per_kimg', (tick_end_time - tick_start_time) / (cur_nimg - tick_start_nimg) * 1e3):<7.2f}\"]\n",
    "        fields += [f\"maintenance {training_stats.report0('Timing/maintenance_sec', maintenance_time):<6.1f}\"]\n",
    "        fields += [f\"cpumem {training_stats.report0('Resources/cpu_mem_gb', psutil.Process(os.getpid()).memory_info().rss / 2**30):<6.2f}\"]\n",
    "        fields += [f\"gpumem {training_stats.report0('Resources/peak_gpu_mem_gb', torch.cuda.max_memory_allocated(device) / 2**30):<6.2f}\"]\n",
    "        fields += [f\"reserved {training_stats.report0('Resources/peak_gpu_mem_reserved_gb', torch.cuda.max_memory_reserved(device) / 2**30):<6.2f}\"]\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        fields += [f\"augment {training_stats.report0('Progress/augment', float(augment_pipe.p.cpu()) if augment_pipe is not None else 0):.3f}\"]\n",
    "        training_stats.report0('Progress/lr', cur_lr)\n",
    "        training_stats.report0('Progress/ema_mimg', cur_ema_nimg / 1e6)\n",
    "        training_stats.report0('Progress/beta2', cur_beta2)\n",
    "        training_stats.report0('Progress/gamma', cur_gamma)\n",
    "        training_stats.report0('Timing/total_hours', (tick_end_time - start_time) / (60 * 60))\n",
    "        training_stats.report0('Timing/total_days', (tick_end_time - start_time) / (24 * 60 * 60))\n",
    "        if rank == 0:\n",
    "            print(' '.join(fields))\n",
    "\n",
    "        # Check for abort.\n",
    "        if (not done) and (abort_fn is not None) and abort_fn():\n",
    "            done = True\n",
    "            if rank == 0:\n",
    "                print()\n",
    "                print('Aborting...')\n",
    "\n",
    "        # Save image snapshot.\n",
    "        if (rank == 0) and (image_snapshot_ticks is not None) and (done or cur_tick % image_snapshot_ticks == 0):\n",
    "            images = torch.cat([G_ema(z, c).cpu() for z, c in zip(grid_z, grid_c)]).to(torch.float).numpy()\n",
    "            save_image_grid(images, os.path.join(run_dir, f'fakes{cur_nimg//1000:09d}.png'), drange=[-1,1], grid_size=grid_size)\n",
    "\n",
    "        # Save network snapshot.\n",
    "        snapshot_pkl = None\n",
    "        snapshot_data = None\n",
    "        if (network_snapshot_ticks is not None) and (done or cur_tick % network_snapshot_ticks == 0):\n",
    "            snapshot_data = dict(G=G, D=D, G_ema=G_ema, training_set_kwargs=dict(training_set_kwargs), cur_nimg=cur_nimg)\n",
    "            for phase in phases:\n",
    "                snapshot_data[phase.name + '_opt_state'] = remap_optimizer_state_dict(phase.opt.state_dict(), 'cpu')\n",
    "            for key, value in snapshot_data.items():\n",
    "                if isinstance(value, torch.nn.Module):\n",
    "                    value = copy.deepcopy(value).eval().requires_grad_(False)\n",
    "                    if num_gpus > 1:\n",
    "                        misc.check_ddp_consistency(value, ignore_regex=r'.*\\.[^.]+_(avg|ema)')\n",
    "                        for param in misc.params_and_buffers(value):\n",
    "                            torch.distributed.broadcast(param, src=0)\n",
    "                    snapshot_data[key] = value.cpu()\n",
    "                del value # conserve memory\n",
    "            snapshot_pkl = os.path.join(run_dir, f'network-snapshot-{cur_nimg//1000:09d}.pkl')\n",
    "            if rank == 0:\n",
    "                with open(snapshot_pkl, 'wb') as f:\n",
    "                    pickle.dump(snapshot_data, f)\n",
    "\n",
    "        # Evaluate metrics.\n",
    "        if (snapshot_data is not None) and (len(metrics) > 0):\n",
    "            if rank == 0:\n",
    "                print('Evaluating metrics...')\n",
    "            for metric in metrics:\n",
    "                result_dict = metric_main.calc_metric(metric=metric, G=snapshot_data['G_ema'],\n",
    "                    dataset_kwargs=training_set_kwargs, num_gpus=num_gpus, rank=rank, device=device)\n",
    "                if rank == 0:\n",
    "                    metric_main.report_metric(result_dict, run_dir=run_dir, snapshot_pkl=snapshot_pkl)\n",
    "                stats_metrics.update(result_dict.results)\n",
    "        del snapshot_data # conserve memory\n",
    "\n",
    "        # Collect statistics.\n",
    "        for phase in phases:\n",
    "            value = []\n",
    "            if (phase.start_event is not None) and (phase.end_event is not None):\n",
    "                phase.end_event.synchronize()\n",
    "                value = phase.start_event.elapsed_time(phase.end_event)\n",
    "            training_stats.report0('Timing/' + phase.name, value)\n",
    "        stats_collector.update()\n",
    "        stats_dict = stats_collector.as_dict()\n",
    "\n",
    "        # Update logs.\n",
    "        timestamp = time.time()\n",
    "        if stats_jsonl is not None:\n",
    "            fields = dict(stats_dict, timestamp=timestamp)\n",
    "            stats_jsonl.write(json.dumps(fields) + '\\n')\n",
    "            stats_jsonl.flush()\n",
    "        if stats_tfevents is not None:\n",
    "            global_step = int(cur_nimg / 1e3)\n",
    "            walltime = timestamp - start_time\n",
    "            for name, value in stats_dict.items():\n",
    "                stats_tfevents.add_scalar(name, value.mean, global_step=global_step, walltime=walltime)\n",
    "            for name, value in stats_metrics.items():\n",
    "                stats_tfevents.add_scalar(f'Metrics/{name}', value, global_step=global_step, walltime=walltime)\n",
    "            stats_tfevents.flush()\n",
    "        if progress_fn is not None:\n",
    "            progress_fn(cur_nimg // 1000, total_kimg)\n",
    "\n",
    "        # Update state.\n",
    "        cur_tick += 1\n",
    "        tick_start_nimg = cur_nimg\n",
    "        tick_start_time = time.time()\n",
    "        maintenance_time = tick_start_time - tick_end_time\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Done.\n",
    "    if rank == 0:\n",
    "        print()\n",
    "        print('Exiting...')\n",
    "\n",
    "#----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n",
      "\n",
      "Num images:  33\n",
      "Image shape: [64, 576]\n",
      "Label shape: 1\n",
      "\n",
      "Constructing networks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xelpmoc\\AppData\\Local\\Temp\\ipykernel_15480\\3596480369.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor = torch.load(file_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeneratorModel                   Parameters  Buffers  Output shape       Datatype\n",
      "---                              ---         ---      ---                ---     \n",
      "generator.MainLayers.0.Layers.0  34816       -        [4, 128, 4, 4]     float32 \n",
      "generator.MainLayers.0.Layers.1  655872      -        [4, 128, 4, 4]     float32 \n",
      "generator.MainLayers.0.Layers.2  655872      -        [4, 128, 4, 4]     float32 \n",
      "generator.MainLayers.1.Layers.0  8192        16       [4, 64, 8, 8]      float32 \n",
      "generator.MainLayers.1.Layers.1  164096      -        [4, 64, 8, 8]      float32 \n",
      "generator.MainLayers.1.Layers.2  164096      -        [4, 64, 8, 8]      float32 \n",
      "generator.MainLayers.2.Layers.0  2048        16       [4, 32, 16, 16]    float32 \n",
      "generator.MainLayers.2.Layers.1  41088       -        [4, 32, 16, 16]    float32 \n",
      "generator.MainLayers.2.Layers.2  41088       -        [4, 32, 16, 16]    float32 \n",
      "generator.MainLayers.3.Layers.0  512         16       [4, 16, 32, 32]    float32 \n",
      "generator.MainLayers.3.Layers.1  10304       -        [4, 16, 32, 32]    float32 \n",
      "generator.MainLayers.3.Layers.2  10304       -        [4, 16, 32, 32]    float32 \n",
      "generator.MainLayers.4.Layers.0  16          16       [4, 1, 64, 64]     float32 \n",
      "generator.MainLayers.4.Layers.1  44          -        [4, 1, 64, 64]     float32 \n",
      "generator.MainLayers.4.Layers.2  44          -        [4, 1, 64, 64]     float32 \n",
      "generator.AggregationLayer       3           -        [4, 3, 64, 64]     float32 \n",
      "upsampler.conv1                  1792        -        [4, 64, 64, 64]    float32 \n",
      "upsampler.conv2                  73856       -        [4, 128, 64, 64]   float32 \n",
      "upsampler.upsample1              32896       -        [4, 128, 64, 128]  float32 \n",
      "upsampler.upsample2              16448       -        [4, 64, 64, 256]   float32 \n",
      "upsampler.upsample3              4128        -        [4, 32, 64, 512]   float32 \n",
      "upsampler.upsample4              33          -        [4, 1, 64, 512]    float32 \n",
      "upsampler.final_conv             295488      -        [4, 1, 64, 576]    float32 \n",
      "---                              ---         ---      ---                ---     \n",
      "Total                            2213036     64       -                  -       \n",
      "\n",
      "\n",
      "DiscriminatorModel                   Parameters  Buffers  Output shape       Datatype\n",
      "---                                  ---         ---      ---                ---     \n",
      "imagedownsampler.final_conv          295424      -        [4, 1, 64, 512]    float32 \n",
      "imagedownsampler.upsample4           64          -        [4, 32, 64, 512]   float32 \n",
      "imagedownsampler.upsample3           4160        -        [4, 64, 64, 256]   float32 \n",
      "imagedownsampler.upsample2           16512       -        [4, 128, 64, 128]  float32 \n",
      "imagedownsampler.upsample1           32896       -        [4, 128, 64, 64]   float32 \n",
      "imagedownsampler.conv2               73792       -        [4, 64, 64, 64]    float32 \n",
      "imagedownsampler.conv1               1731        -        [4, 3, 64, 64]     float32 \n",
      "imagedownsampler                     -           -        [4, 3, 64, 64]     float32 \n",
      "discriminator.ExtractionLayer        3           -        [4, 1, 64, 64]     float32 \n",
      "discriminator.MainLayers.0.Layers.0  44          -        [4, 1, 64, 64]     float32 \n",
      "discriminator.MainLayers.0.Layers.1  44          -        [4, 1, 64, 64]     float32 \n",
      "discriminator.MainLayers.0.Layers.2  16          16       [4, 16, 32, 32]    float32 \n",
      "discriminator.MainLayers.1.Layers.0  10304       -        [4, 16, 32, 32]    float32 \n",
      "discriminator.MainLayers.1.Layers.1  10304       -        [4, 16, 32, 32]    float32 \n",
      "discriminator.MainLayers.1.Layers.2  512         16       [4, 32, 16, 16]    float32 \n",
      "discriminator.MainLayers.2.Layers.0  41088       -        [4, 32, 16, 16]    float32 \n",
      "discriminator.MainLayers.2.Layers.1  41088       -        [4, 32, 16, 16]    float32 \n",
      "discriminator.MainLayers.2.Layers.2  2048        16       [4, 64, 8, 8]      float32 \n",
      "discriminator.MainLayers.3.Layers.0  164096      -        [4, 64, 8, 8]      float32 \n",
      "discriminator.MainLayers.3.Layers.1  164096      -        [4, 64, 8, 8]      float32 \n",
      "discriminator.MainLayers.3.Layers.2  8192        16       [4, 128, 4, 4]     float32 \n",
      "discriminator.MainLayers.4.Layers.0  655872      -        [4, 128, 4, 4]     float32 \n",
      "discriminator.MainLayers.4.Layers.1  655872      -        [4, 128, 4, 4]     float32 \n",
      "discriminator.MainLayers.4.Layers.2  2176        -        [4, 1]             float32 \n",
      "discriminator                        -           -        [4]                float32 \n",
      "---                                  ---         ---      ---                ---     \n",
      "Total                                2180334     64       -                  -       \n",
      "\n",
      "Setting up augmentation...\n",
      "Distributing across 1 GPUs...\n",
      "Setting up training phases...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tried to instantiate dummy base class Event",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ema_nimg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      2\u001b[0m decay_nimg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e7\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtraining_set_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mXelpmoc\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mR3GAN\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_state_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mXelpmoc\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mR3GAN\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mhidden_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdata_loader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnnlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEasyDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefetch_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mG_opt_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnnlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEasyDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch.optim.Adam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mD_opt_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnnlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEasyDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch.optim.Adam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_nimg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_nimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbeta2_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_nimg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_nimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m              \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnnlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEasyDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining.loss.R3GANLoss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mgamma_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_nimg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_nimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfid50k_full\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m              \u001b[49m\u001b[43mema_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_nimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_nimg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_nimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 222\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(run_dir, training_set_kwargs, data_loader_kwargs, G_opt_kwargs, D_opt_kwargs, lr_scheduler, beta2_scheduler, augment_kwargs, loss_kwargs, gamma_scheduler, metrics, random_seed, num_gpus, rank, batch_size, g_batch_gpu, d_batch_gpu, ema_scheduler, aug_scheduler, total_kimg, kimg_per_tick, image_snapshot_ticks, network_snapshot_ticks, resume_pkl, cudnn_benchmark, abort_fn, progress_fn)\u001b[0m\n\u001b[0;32m    220\u001b[0m     phase\u001b[38;5;241m.\u001b[39mend_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 222\u001b[0m         phase\u001b[38;5;241m.\u001b[39mstart_event \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menable_timing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m         phase\u001b[38;5;241m.\u001b[39mend_event \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mEvent(enable_timing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Export sample images.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xelpmoc\\miniconda3\\envs\\env\\Lib\\site-packages\\torch\\cuda\\streams.py:165\u001b[0m, in \u001b[0;36mEvent.__new__\u001b[1;34m(cls, enable_timing, blocking, interprocess)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enable_timing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, interprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_timing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_timing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xelpmoc\\miniconda3\\envs\\env\\Lib\\site-packages\\torch\\_utils.py:923\u001b[0m, in \u001b[0;36m_dummy_type.<locals>.get_err_fn.<locals>.err_fn\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    922\u001b[0m     class_name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 923\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to instantiate dummy base class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tried to instantiate dummy base class Event"
     ]
    }
   ],
   "source": [
    "ema_nimg = 5000 * 1000\n",
    "decay_nimg = 2e7\n",
    "training_loop(run_dir=\"./\",\n",
    "              training_set_kwargs={\"embedding_path\":\"D:\\\\Xelpmoc\\\\R3GAN\\\\embeddings\",\"hidden_state_path\":\"D:\\\\Xelpmoc\\\\R3GAN\\\\hidden_states\"},\n",
    "              data_loader_kwargs=dnnlib.EasyDict(pin_memory=True, prefetch_factor=2,num_workers=2),\n",
    "              G_opt_kwargs=dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0,0], eps=1e-8),\n",
    "              D_opt_kwargs=dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0,0], eps=1e-8),\n",
    "              lr_scheduler={ 'base_value': 2e-4, 'final_value': 5e-5, 'total_nimg': decay_nimg },\n",
    "              beta2_scheduler={ 'base_value': 0.9, 'final_value': 0.99, 'total_nimg': decay_nimg },\n",
    "              loss_kwargs=dnnlib.EasyDict(class_name='training.loss.R3GANLoss'),\n",
    "              gamma_scheduler={ 'base_value': 0.05, 'final_value': 0.005, 'total_nimg': decay_nimg },\n",
    "              metrics=['fid50k_full'],\n",
    "              ema_scheduler={ 'base_value': 0, 'final_value': ema_nimg, 'total_nimg': decay_nimg })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
